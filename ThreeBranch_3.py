import torch
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import torch.nn.functional as F
import math

#changed the G generated by itself and add the mask to guide the Infor part



### Multistage dual-attention guided fusion network
class MSDANet(nn.Module):
    def __init__(self, Dim=[3, 34, 31], depth=1, nDenselayer=4, nFeat=64, growthRate=32):
        super(MSDANet, self).__init__()

        block1_1 = []
        block1_2 = []
        block1_3 = []
        block2_1 = []
        block2_2 = []
        block2_3 = []
        block3_1 = []
        block3_2 = []
        block3_3 = []

        for i in range(depth):
            block1_1.append(RWDB(nFeat, nDenselayer, growthRate))
            block1_2.append(RWDB(nFeat, nDenselayer, growthRate))
            block1_3.append(RWDB(nFeat, nDenselayer, growthRate))
            block2_1.append(RWDB(nFeat, nDenselayer, growthRate))
            block2_2.append(RWDB(nFeat, nDenselayer, growthRate))
            block2_3.append(RWDB(nFeat, nDenselayer, growthRate))
            block3_1.append(RWDB(nFeat, nDenselayer+1, growthRate))
            block3_2.append(RWDB(nFeat, nDenselayer+1, growthRate))
            block3_3.append(RWDB(nFeat, nDenselayer+1, growthRate))

        self.conv1_1 = nn.Sequential(*[nn.Conv2d(Dim[0], nFeat, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv2d(nFeat, nFeat, 3, 1, 1)])
        self.conv2_1 = nn.Sequential(*[nn.Conv2d(Dim[2], nFeat, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv2d(nFeat, nFeat, 3, 1, 1)])
        self.conv3_1 = nn.Sequential(*[nn.Conv2d(Dim[1], nFeat, 3, 1, 1), nn.ReLU(inplace=True), nn.Conv2d(nFeat, nFeat, 3, 1, 1)])

        self.da1 = DABlock(nFeat)
        self.da2 = DABlock(nFeat)
        self.da3 = DABlock(nFeat)
        self.da4 = DABlock(nFeat)


        self.branch1_1 = nn.Sequential(*block1_1)
        self.branch1_2 = nn.Sequential(*block1_2)
        self.branch1_3 = nn.Sequential(*block1_3)
        self.branch2_1 = nn.Sequential(*block2_1)
        self.branch2_2 = nn.Sequential(*block2_2)
        self.branch2_3 = nn.Sequential(*block2_3)
        self.branch3_1 = nn.Sequential(*block3_1)
        self.branch3_2 = nn.Sequential(*block3_2)
        self.branch3_3 = nn.Sequential(*block3_3)
        self.branch_out = RWDB(nFeat, nDenselayer+1, growthRate)


        self.conv2 = nn.Conv2d(nFeat*3, nFeat, 3, 1, 1)
        self.conv3 = nn.Conv2d(nFeat, nFeat, 3, 1, 1)

        self.conv4 = nn.Conv2d(nFeat, nFeat, 3, 1, 1)
        self.conv5 = nn.Conv2d(nFeat, Dim[2], 3, 1 ,1)

        self.Relu = nn.ReLU(inplace=True)
        self.Sig = nn.Sigmoid()

    def forward(self, Input):
        [x, z, y] = Input
        res = y

        ### Input stage
        out1 = self.conv1_1(x)
        out2 = self.conv2_1(y)
        out3 = self.conv3_1(z)
        res1 = out1
        res2 = out2
        res3 = out3

        out1 = self.branch1_1(out1)
        out2 = self.branch2_1(out2)
        out3 = self.branch3_1(out3)
        out3 = self.da1(out1, out3, out2)

        out1 = self.branch1_2(out1)
        out2 = self.branch2_2(out2)
        out3 = self.branch3_2(out3)
        out3 = self.da2(out1, out3, out2)

        out1 = self.branch1_3(out1)
        out2 = self.branch2_3(out2)
        out3 = self.branch3_3(out3)
        out3 = self.da3(out1, out3, out2)

        out1 = out1 + res1
        out2 = out2 + res2
        out3 = out3 + res3

        ### Reconstruction stage
        out = self.Relu(self.conv2(torch.cat((out1, out3, out2), 1)))
        out = self.branch_out(out)
        out = self.conv3(out) + res

        return out
